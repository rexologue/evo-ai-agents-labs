######################
# FILE: .env.example #
######################

# Настройки MCP
DB_MCP_URL=http://db-mcp:8000
CODES_MCP_URL=http://codes-mcp:8001

# Agent Configuration
AGENT_NAME=CompanyProfiler
AGENT_DESCRIPTION="LangChain Agent для формирования профиля компаний"
AGENT_HOST=0.0.0.0
AGENT_PORT=8002
AGENT_URL=https://your-agent-id-agent.ai-agent.inference.cloud.ru
AGENT_VERSION=v1.0.0

# Обязательные для агента переменные
LLM_API_KEY=your_openai_key
LLM_MODEL=gpt-4o-mini
LLM_API_BASE=https://api.openai.com/v1


####################
# FILE: Dockerfile #
####################

# Используем официальный образ Python 3.13 slim для минимального размера
FROM python:3.13-slim

# Устанавливаем рабочую директорию
WORKDIR /app

# Устанавливаем системные зависимости и uv в одном слое для оптимизации
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && pip install --no-cache-dir uv

# Копируем файлы конфигурации uv сначала для лучшего кеширования Docker слоев
COPY agent-profiler/pyproject.toml ./

# Создаем виртуальное окружение и устанавливаем зависимости
# Используем тот же подход, что и локально
RUN uv sync --no-editable

# Копируем исходный код приложения
COPY agent-profiler/src/ ./src/
COPY models.py ./src/models.py

# Создаем непривилегированного пользователя для безопасности
RUN useradd --create-home --shell /bin/bash --uid 1000 mcp && \
    chown -R mcp:mcp /app

# Переключаемся на непривилегированного пользователя
USER mcp

# Открываем порт для A2A сервера агента (AGENT_PORT по умолчанию 8001)
EXPOSE 8001

# Устанавливаем переменные окружения для оптимальной работы Python
ENV PYTHONPATH=/app \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONHASHSEED=random \
    UV_SYSTEM_PYTHON=1

# Команда запуска сервера - точно так же, как локально
CMD ["uv", "run", "python", "src/start_a2a.py"]


########################
# FILE: pyproject.toml #
########################

[project]
name = "agent-service"
version = "0.1.0"
description = "ProfileAgent for classifying company descriptions and storing via db-mcp"
authors = [{name = "Evo AI Agents Labs"}]
requires-python = ">=3.11"
dependencies = [
    "fastmcp>=0.5.0",
    "pydantic>=2.6",
    "python-dotenv>=1.0",
    "rapidfuzz>=3.9",
    "langchain>=0.3.0,<1.0.0",
    "langchain-core>=0.3.0,<1.0.0",
    "langchain-openai>=0.2.0,<1.0.0",
    "langchain_mcp_adapters",
    "a2a-sdk>=0.3.4,<0.4.0",
    "litellm>=1.76.3,<2.0.0",
    "openinference-instrumentation-langchain>=0.1.0,<0.2.0",
    "arize-phoenix-otel>=0.13.0,<0.14.0",
    "httpx>=0.27.0,<1.0.0",
    "uvicorn>=0.30.0,<1.0.0",
]

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"


#########################
# FILE: src/__init__.py #
#########################



############################
# FILE: src/a2a_wrapper.py #
############################

"""Обертка LangChain агента для A2A протокола."""
import re
import asyncio
import logging
from typing import Dict, Any, AsyncGenerator, List

from langchain.agents import AgentExecutor
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage

logger = logging.getLogger(__name__)


def _strip_need_input(text: str) -> tuple[str, bool]:
    """
    Убирает тег `<NEED_USER_INPUT>` и возвращает (очищенный_текст, нужен_ли_ответ_пользователя).
    """
    if not text:
        return text, False

    if "<NEED_USER_INPUT>" in text:
        clean = text.replace("<NEED_USER_INPUT>", "").strip()
        return clean, True

    return text, False


class LangChainA2AWrapper:
    """Обертка для преобразования LangChain агента в A2A-совместимый интерфейс."""

    # Для совместимости с A2A
    SUPPORTED_CONTENT_TYPES = ["text", "text/plain"]

    def __init__(self, agent_executor: AgentExecutor, auto_reset_on_complete: bool = True):
        self.agent_executor = agent_executor
        # ✅ история как список BaseMessage (HumanMessage / AIMessage)
        self.sessions: Dict[str, List[BaseMessage]] = {}
        # ✅ включаем/выключаем автоочистку сессии после завершения задачи
        self.auto_reset_on_complete = auto_reset_on_complete

    def _get_session_history(self, session_id: str) -> List[BaseMessage]:
        """Получает историю сессии для подстановки в MessagesPlaceholder(chat_history)."""
        if session_id not in self.sessions:
            self.sessions[session_id] = []
        return self.sessions[session_id]

    def _reset_session(self, session_id: str) -> None:
        """Полностью сбрасывает историю для сессии (логическое завершение контекста)."""
        if session_id in self.sessions:
            del self.sessions[session_id]
            logger.debug("Session %s has been reset (task complete).", session_id)

    async def invoke(self, query: str, session_id: str) -> Dict[str, Any]:
        """
        Синхронный (не-стриминговый) вызов агента.

        - чистим <think>...</think>
        - обрабатываем <NEED_USER_INPUT>
        - обновляем историю
        - при необходимости сбрасываем сессию после завершения задачи
        """
        try:
            chat_history = self._get_session_history(session_id)

            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                lambda: self.agent_executor.invoke(
                    {
                        "input": query,
                        "chat_history": chat_history,
                    }
                ),
            )

            if isinstance(result, dict):
                raw_output = result.get("output", "")
            else:
                raw_output = str(result)

            # Убираем служебные теги
            clean_output = _strip_think_blocks(raw_output)
            clean_output, need_input = _strip_need_input(clean_output)

            # Обновляем историю диалога
            if query:
                chat_history.append(HumanMessage(content=query))
            if clean_output:
                chat_history.append(AIMessage(content=clean_output))

            response = {
                "is_task_complete": not need_input,
                "require_user_input": need_input,
                "content": clean_output,
                "is_error": False,
                "is_event": False,
            }

            # ✅ если задача завершена и не требуется ввод пользователя — чистим сессию
            if self.auto_reset_on_complete and not need_input:
                self._reset_session(session_id)

            return response

        except Exception as e:
            logger.exception("Error in LangChainA2AWrapper.invoke")
            return {
                "is_task_complete": True,
                "require_user_input": False,
                "content": f"Ошибка: {str(e)}",
                "is_error": True,
                "is_event": False,
            }

    async def stream(
        self,
        query: str,
        session_id: str,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Стриминговый вызов агента.

        - копим полный ответ в full_response
        - в конце чистим <think> и <NEED_USER_INPUT>
        - обновляем историю
        - по завершении, если не нужен ввод пользователя — сбрасываем сессию
        """
        try:
            chat_history = self._get_session_history(session_id)
            full_response = ""

            # LangChain AgentExecutor поддерживает astream
            async for chunk in self.agent_executor.astream(
                {
                    "input": query,
                    "chat_history": chat_history,
                }
            ):
                if not isinstance(chunk, dict):
                    continue

                # Основной поток текста
                if "output" in chunk and isinstance(chunk["output"], str):
                    delta = chunk["output"]
                    if delta:
                        full_response += delta
                        yield {
                            "is_task_complete": False,
                            "require_user_input": False,
                            "content": delta,
                            "is_error": False,
                            "is_event": False,
                        }

                # Промежуточные события о вызове инструментов
                if "intermediate_steps" in chunk:
                    for step in chunk["intermediate_steps"] or []:
                        try:
                            tool_name = getattr(step[0], "tool", "tool")
                        except Exception:
                            tool_name = "tool"
                        yield {
                            "is_task_complete": False,
                            "require_user_input": False,
                            "content": f"Использую инструмент: {tool_name}\n",
                            "is_error": False,
                            "is_event": True,
                        }

            # После окончания стрима чистим служебные теги
            clean_full = _strip_think_blocks(full_response)
            clean_full, need_input = _strip_need_input(clean_full)

            # Обновляем историю диалога
            if query:
                chat_history.append(HumanMessage(content=query))
            if clean_full:
                chat_history.append(AIMessage(content=clean_full))

            # Готовим финальный payload
            if not full_response or clean_full != full_response:
                final_payload = {
                    "is_task_complete": not need_input,
                    "require_user_input": need_input,
                    "content": clean_full,
                    "is_error": False,
                    "is_event": False,
                }
            else:
                final_payload = {
                    "is_task_complete": not need_input,
                    "require_user_input": need_input,
                    "content": "",
                    "is_error": False,
                    "is_event": False,
                }

            # ✅ Автоочистка сессии, если задача завершена и не нужен ввод пользователя
            if self.auto_reset_on_complete and not need_input:
                self._reset_session(session_id)

            # Отдаём финальное сообщение
            yield final_payload

        except Exception as e:
            logger.exception("Error in LangChainA2AWrapper.stream")
            yield {
                "is_task_complete": True,
                "require_user_input": False,
                "content": f"Ошибка: {str(e)}",
                "is_error": True,
                "is_event": False,
            }


######################
# FILE: src/agent.py #
######################

"""Определение LangChain агента с поддержкой MCP инструментов и классификацией по ОКПД2."""

import asyncio
from typing import List, Optional, Dict, Sequence

from langchain_openai import ChatOpenAI
from langchain_core.tools import BaseTool
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from langchain_mcp_adapters.client import MultiServerMCPClient  # MCP <-> LangChain

from config import get_settings
from base_prompt import BASE_SYSTEM_PROMPT

settings = get_settings()


def _normalize_mcp_url(raw: str) -> str:
    """Нормализует URL MCP сервера к виду .../mcp."""
    raw = (raw or "").strip()
    if not raw:
        raise ValueError("Пустой MCP URL")

    # уже /mcp или /mcp/
    if raw.rstrip("/").endswith("/mcp"):
        return raw.rstrip("/")

    return raw.rstrip("/") + "/mcp"


def _build_mcp_client(mcp_urls: Optional[str]) -> Optional[MultiServerMCPClient]:
    """
    Создаёт MultiServerMCPClient по строке с MCP URL-ами.

    Поддерживаем форматы:
      - "http://db-mcp:28001/mcp"
      - "http://db-mcp:28001"
      - "finance=http://db-mcp:28001/mcp"
      - "finance=http://db-mcp:28001,gosplan=http://gosplan-mcp:28002/mcp"
    """
    if not mcp_urls:
        return None

    servers: Dict[str, dict] = {}

    for idx, item in enumerate(mcp_urls.split(",")):
        item = item.strip()
        if not item:
            continue

        if "=" in item:
            name, url = item.split("=", 1)
            name = name.strip() or f"mcp_{idx}"
            url = url.strip()
        else:
            name = f"mcp_{idx}"
            url = item.strip()

        if not url:
            continue

        url = _normalize_mcp_url(url)

        servers[name] = {
            "transport": "streamable_http",  # streamable HTTP поверх FastMCP
            "url": url,
        }

    if not servers:
        return None

    return MultiServerMCPClient(servers)


async def _get_mcp_tools_async(mcp_urls: Optional[str]) -> List[BaseTool]:
    """Асинхронная загрузка всех тулов со всех MCP-серверов."""
    client = _build_mcp_client(mcp_urls)
    if client is None:
        return []

    tools = await client.get_tools()
    return list(tools)


def get_mcp_tools(mcp_urls: Optional[str]) -> List[BaseTool]:
    """Синхронная обёртка над асинхронной загрузкой MCP-тулов."""
    if not mcp_urls:
        return []
    return asyncio.run(_get_mcp_tools_async(mcp_urls))


def create_langchain_agent(
    mcp_urls: str | list[str] | None = None
) -> AgentExecutor:
    """Создает LangChain агента с MCP инструментами"""
    # LLM
    llm = ChatOpenAI(
        model=settings.llm_model,
        base_url=settings.llm_api_base,
        api_key=settings.llm_api_key,
        temperature=0.1,
    )

    # Инструменты MCP (db-mcp и др.)
    mcp_tools = []
    
    if isinstance(mcp_urls, list):
        for url in mcp_urls:
            mcp_tools.extend(get_mcp_tools(url))   
    elif isinstance(mcp_urls, str):
        mcp_tools.extend(get_mcp_tools(mcp_urls))
    else:
        pass

    # Системный промпт задается статически через base_prompt
    system_prompt = BASE_SYSTEM_PROMPT

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "{system_prompt}"),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ]
    ).partial(
        system_prompt=system_prompt
    )

    agent = create_openai_tools_agent(llm, mcp_tools, prompt)

    agent_executor = AgentExecutor(
        agent=agent,
        tools=mcp_tools,
        verbose=True,
        handle_parsing_errors=True,
        max_iterations=20,
    )

    return agent_executor


###################################
# FILE: src/agent_task_manager.py #
###################################

"""AgentExecutor для интеграции LangChain агента с A2A."""
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.server.tasks import TaskUpdater
from a2a.types import (
    Task,
    TaskState,
    UnsupportedOperationError,
)
from a2a.utils import (
    new_agent_text_message,
    new_task,
)
from a2a.utils.errors import ServerError
from a2a_wrapper import LangChainA2AWrapper


class LangChainAgentExecutor(AgentExecutor):
    """AgentExecutor для LangChain агента."""

    def __init__(self, agent_wrapper: LangChainA2AWrapper):
        self.agent = agent_wrapper

    async def execute(
        self,
        context: RequestContext,
        event_queue: EventQueue,
    ) -> None:
        query = context.get_user_input()
        task = context.current_task

        # Создаем новую задачу, если её нет
        if not task:
            task = new_task(context.message)
            await event_queue.enqueue_event(task)
        
        updater = TaskUpdater(event_queue, task.id, task.context_id)
        
        # Вызываем агента с streaming
        async for item in self.agent.stream(query, task.context_id):
            is_task_complete = item['is_task_complete']
            require_user_input = item['require_user_input']
            is_error = item['is_error']
            is_event = item['is_event']

            if is_error:
                await updater.update_status(
                    TaskState.failed,
                    new_agent_text_message(
                        item['content'], task.context_id, task.id
                    ),
                )
                break
            
            if is_event:
                await updater.update_status(
                    TaskState.working,
                    new_agent_text_message(
                        item['content'], task.context_id, task.id
                    ),
                )
                continue
            
            if not is_task_complete and not require_user_input:
                await updater.update_status(
                    TaskState.working,
                    new_agent_text_message(
                        item['content'], task.context_id, task.id
                    ),
                )
                continue

            if not is_task_complete and require_user_input:
                await updater.update_status(
                    TaskState.input_required,
                    new_agent_text_message(
                        item['content'], task.context_id, task.id
                    ),
                )
                break
            
            if is_task_complete and not require_user_input:
                await updater.update_status(
                    TaskState.completed,
                    new_agent_text_message(
                        item['content'], task.context_id, task.id
                    ),
                )
                break

    async def cancel(
        self, request: RequestContext, event_queue: EventQueue
    ) -> Task | None:
        raise ServerError(error=UnsupportedOperationError())




############################
# FILE: src/base_prompt.py #
############################

from __future__ import annotations

import json
from pydantic import BaseModel

from models import CompanyProfileBase, Okpd2Item


def _format_fields_for_prompt(model: type[BaseModel]) -> str:
    """Генерирует человекочитаемый список полей для промпта на основе Pydantic-модели."""
    lines: list[str] = []
    
    for name, field in model.model_fields.items():
        required = field.is_required()
        required_mark = "required" if required else "optional"
        desc = field.description or "(нет описания)"
        type_info = str(field.annotation)
        lines.append(f"- `{name}` ({required_mark}, type: {type_info}) — {desc}")
        
    return "\n".join(lines)


def _build_example_json(model: type[BaseModel]) -> str:
    """Генерирует небольшой пример JSON-профиля для промпта."""
    example = model(
        name="ООО «Честный взгляд»",
        description="Компания из Красноярска, производит кухонную мебель и выполняет установку кухонь.",
        regions=["Красноярск"],
        min_contract_price=100_000,
        max_contract_price=3_000_000,
        industries=["производство мебели", "установка кухонь", "мебель под заказ"],
        resources=["собственный производственный цех", "сеть магазинов по Красноярску"],
        risk_tolerance="low",
        okpd2_codes=[
            Okpd2Item(code="31.02", title="Производство кухонной мебели"),
            Okpd2Item(code="43.32", title="Установка столярных изделий"),
        ],
    )
    return json.dumps(example.model_dump(), ensure_ascii=False, indent=2)


_BASE_SYSTEM_PROMPT_TEMPLATE = """
You are the "CompanyProfiler" agent.

Your purpose:
Given a free-form description of a company in Russian, you must:
1. Collect all required fields for a structured company profile.
2. If some required fields are missing or ambiguous, ask the user clarifying questions in Russian.
3. When you have enough information, build a clear draft profile in Russian and show it to the user for approval.
4. Only after the user explicitly confirms that the draft profile is correct, call the MCP tool to save the profile to the database.
5. In the final message after saving, always show the final profile and the assigned company ID in Russian.

---

## Language policy

- ALWAYS communicate with the user in Russian.
- All textual fields in the profile MUST be written in Russian,
  except company names/brands that are originally written in another language.
- Do not switch to English in your answers or field values unless the user explicitly asks you to.

---

## Company profile schema (Pydantic model)

You must build a profile that matches the following Pydantic model fields:

{fields_spec}

Here is an example JSON object that follows this schema:

```json
{example_json}
```
You MUST respect the field names exactly as shown above. Do NOT invent new top-level fields.

Interaction phases
Phase 1 — Data collection
Read the user’s description of the company (in Russian).

Determine which required fields are already known and which are missing or ambiguous.

Ask focused clarifying questions in Russian ONLY about the fields you still need.

While you are waiting for the user’s answer:

At the very end of your message, on a separate line, add the tag:
<NEED_USER_INPUT>

In this case you MUST NOT call any database-saving MCP tool.

Repeat this phase until all required fields from the schema are reliably filled.

Phase 2 — Draft profile and user approval
When you believe all required fields from the schema are filled:

DO NOT call the database MCP tool yet.

Prepare a clear draft company profile in Russian. A recommended structure:

Заголовок: Черновик профиля компании "<Название>"

Блок "Основная информация" (описание, регионы, отрасли).

Блок "Финансовые параметры" (min/max контрактов).

Блок "Ресурсы" (ключевые ресурсы компании).

Блок "Уровень риска" (объясни в 1–2 предложениях, почему выбран low/medium/high).

Блок "Предварительные коды ОКПД2" (коды и краткие подписи).

To determine okpd2_codes, you MUST use the dedicated OKPD2 helper tool
and choose 1–5 most relevant codes from the tool response. Do NOT invent codes.

Show the draft profile to the user in Russian and explicitly ask them to confirm or correct it.

Since you are waiting for the user’s reply (approval or corrections),
you MUST add <NEED_USER_INPUT> on a separate line at the end of the message.

If the user asks for corrections, update the draft and show an updated draft profile again,
with <NEED_USER_INPUT> at the end, until the user clearly confirms that the profile is correct.

Phase 3 — Saving the profile to the database
Only AFTER the user explicitly confirms (in Russian) that the draft profile is correct

Make sure all required fields from the Pydantic schema are set.

Call the MCP tool that saves the company profile to the database,
passing all required fields including okpd2_codes.

Extract the assigned company ID from the tool result.

Send a final message to the user in Russian which MUST include:

краткое резюме профиля компании;

перечисление кодов ОКПД2;

строку вида: ID профиля: <id>.

In this final message:

Do NOT add <NEED_USER_INPUT>.

Do NOT ask additional questions.

Consider the task fully completed.

Tools
Use the OKPD2 helper tool whenever you need to determine or refine okpd2_codes.

Use the database MCP tool ONLY in Phase 3, after explicit user approval of the draft profile.

If a tool call fails, explain the problem to the user in Russian and ask how they want to proceed.

General style
Be professional, concise, and polite.

Never invent values for fields that were not clearly provided or unambiguously implied.

If something is unclear, ask a focused clarifying question in Russian instead of guessing.

Remember:

<NEED_USER_INPUT> is used ONLY when you are waiting for the user’s reply.

The final message after saving MUST NOT contain <NEED_USER_INPUT> and MUST clearly show the profile ID.
"""


def build_system_prompt() -> str:
    """
    Строит финальный системный промпт на основе Pydantic-модели CompanyProfileBase.
    Если ты поменяешь поля/описания в модели, промпт автоматически обновится.
    """
    fields_spec = _format_fields_for_prompt(CompanyProfileBase)
    example_json = _build_example_json(CompanyProfileBase)

    return _BASE_SYSTEM_PROMPT_TEMPLATE.format(
        fields_spec=fields_spec,
        example_json=example_json,
    )


BASE_SYSTEM_PROMPT = build_system_prompt()


#######################
# FILE: src/config.py #
#######################

import os
import logging
from typing import Optional

from dotenv import load_dotenv, find_dotenv
from pydantic import Field, ValidationError
from pydantic_settings import BaseSettings

# Загрузить .env один раз
load_dotenv(find_dotenv())


# ------------------------------------------------------------------------------
# Логирование
# ------------------------------------------------------------------------------
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
)
logger = logging.getLogger("agent-profiler")


# ------------------------------------------------------------------------------
# Продакшн-класс настроек
# ------------------------------------------------------------------------------
class Settings(BaseSettings):

    # ---- LLM ----
    llm_model: str = Field(..., alias="LLM_MODEL")
    llm_api_key: str = Field(..., alias="LLM_API_KEY")
    llm_api_base: str = Field(..., alias="LLM_API_BASE")

    # ---- MCP ----
    db_mcp_url: str = Field(..., alias="DB_MCP_URL")
    codes_mcp_url: str = Field(..., alias="CODES_MCP_URL")

    # ---- Agent metadata ----
    agent_name: str = Field("CompanyProfiler", alias="AGENT_NAME")
    agent_desc: str = Field(
        "LangChain Agent для формирования профиля компаний",
        alias="AGENT_DESCRIPTION"
    )
    agent_host: str = Field("0.0.0.0", alias="AGENT_HOST")
    agent_port: int = Field(8001, alias="AGENT_PORT")
    agent_url: Optional[str] = Field(None, alias="AGENT_URL")

    agent_version: str = Field("v1.0.0", alias="AGENT_VERSION")

    class Config:
        populate_by_name = True
        extra = "ignore"

    # --------------------------------------------------------------------------
    # Единый post-validator
    # --------------------------------------------------------------------------
    def model_post_init(self, __context):
        # ----------------------------------------------------------
        # AGENT_URL → вычисляем, если не указан
        # ----------------------------------------------------------
        if not self.agent_url:
            self.agent_url = f"http://{self.agent_host}:{self.agent_port}"
            logger.debug(f"agent_url was empty → set to {self.agent_url}")


# ------------------------------------------------------------------------------
# Фабрика
# ------------------------------------------------------------------------------
_settings_cache: Optional[Settings] = None


def get_settings() -> Settings:
    """Singleton-кеш, чтобы не пересоздавать настройки 100 раз"""
    global _settings_cache
    
    if _settings_cache is None:
        try:
            _settings_cache = Settings()
            
        except ValidationError as e:
            logger.error("❌ Invalid configuration:")
            logger.error(e)
            raise
        
    return _settings_cache


##########################
# FILE: src/start_a2a.py #
##########################

"""Точка входа для запуска LangChain агента через A2A протокол."""
from config import get_settings, logger
settings = get_settings()

from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore
from a2a.types import (
    AgentCapabilities,
    AgentCard,
)

from agent import create_langchain_agent
from a2a_wrapper import LangChainA2AWrapper
from agent_task_manager import LangChainAgentExecutor

def main():
    """Основная функция запуска сервера."""
    try:
        # Создаем LangChain агента
        agent_executor = create_langchain_agent([settings.db_mcp_url, settings.codes_mcp_url])
        
        # Создаем A2A обертку
        agent_wrapper = LangChainA2AWrapper(agent_executor)
        
        # Создаем A2A executor
        agent_executor_a2a = LangChainAgentExecutor(agent_wrapper)
        
        # Настройка AgentCard
        capabilities = AgentCapabilities(streaming=True)
        agent_card = AgentCard(
            name=settings.agent_name,
            description=settings.agent_desc,
            url=settings.agent_url,
            version=settings.agent_version,
            default_input_modes=agent_wrapper.SUPPORTED_CONTENT_TYPES,
            default_output_modes=agent_wrapper.SUPPORTED_CONTENT_TYPES,
            capabilities=capabilities,
            skills=[],
        )
        
        # Создаем request handler
        request_handler = DefaultRequestHandler(
            agent_executor=agent_executor_a2a,
            task_store=InMemoryTaskStore(),
        )
        
        # Создаем и запускаем сервер
        server = A2AStarletteApplication(
            agent_card=agent_card,
            http_handler=request_handler
        )
        
        import uvicorn
        logger.info(f"Starting LangChain Agent server on port {settings.agent_port}")
        uvicorn.run(server.build(), host=settings.agent_host, port=settings.agent_port)
        
    except Exception as e:
        logger.error(f'An error occurred during server startup: {e}', exc_info=True)
        exit(1)


if __name__ == '__main__':
    main()

